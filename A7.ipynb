{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Unnamed: 0",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hate_speech",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "offensive_language",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "neither",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "class",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "tweet",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "05e3694a-ab75-41b5-a12d-1fcf153114ab",
       "rows": [
        [
         "0",
         "0",
         "3",
         "0",
         "0",
         "3",
         "2",
         "!!! RT @mayasolovely: As a woman you shouldn't complain about cleaning up your house. &amp; as a man you should always take the trash out..."
        ],
        [
         "1",
         "1",
         "3",
         "0",
         "3",
         "0",
         "1",
         "!!!!! RT @mleew17: boy dats cold...tyga dwn bad for cuffin dat hoe in the 1st place!!"
        ],
        [
         "2",
         "2",
         "3",
         "0",
         "3",
         "0",
         "1",
         "!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby4life: You ever fuck a bitch and she start to cry? You be confused as shit"
        ],
        [
         "3",
         "3",
         "3",
         "0",
         "2",
         "1",
         "1",
         "!!!!!!!!! RT @C_G_Anderson: @viva_based she look like a tranny"
        ],
        [
         "4",
         "4",
         "6",
         "0",
         "6",
         "0",
         "1",
         "!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you hear about me might be true or it might be faker than the bitch who told it to ya &#57361;"
        ],
        [
         "5",
         "5",
         "3",
         "1",
         "2",
         "0",
         "1",
         "!!!!!!!!!!!!!!!!!!\"@T_Madison_x: The shit just blows me..claim you so faithful and down for somebody but still fucking with hoes! &#128514;&#128514;&#128514;\""
        ],
        [
         "6",
         "6",
         "3",
         "0",
         "3",
         "0",
         "1",
         "!!!!!!\"@__BrighterDays: I can not just sit up and HATE on another bitch .. I got too much shit going on!\""
        ],
        [
         "7",
         "7",
         "3",
         "0",
         "3",
         "0",
         "1",
         "!!!!&#8220;@selfiequeenbri: cause I'm tired of you big bitches coming for us skinny girls!!&#8221;"
        ],
        [
         "8",
         "8",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" &amp; you might not get ya bitch back &amp; thats that \""
        ],
        [
         "9",
         "9",
         "3",
         "1",
         "2",
         "0",
         "1",
         "\" @rhythmixx_ :hobbies include: fighting Mariam\"\n\nbitch"
        ],
        [
         "10",
         "10",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" Keeks is a bitch she curves everyone \" lol I walked into a conversation like this. Smh"
        ],
        [
         "11",
         "11",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" Murda Gang bitch its Gang Land \""
        ],
        [
         "12",
         "12",
         "3",
         "0",
         "2",
         "1",
         "1",
         "\" So hoes that smoke are losers ? \" yea ... go on IG"
        ],
        [
         "13",
         "13",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" bad bitches is the only thing that i like \""
        ],
        [
         "14",
         "14",
         "3",
         "1",
         "2",
         "0",
         "1",
         "\" bitch get up off me \""
        ],
        [
         "15",
         "15",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" bitch nigga miss me with it \""
        ],
        [
         "16",
         "16",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" bitch plz whatever \""
        ],
        [
         "17",
         "17",
         "3",
         "1",
         "2",
         "0",
         "1",
         "\" bitch who do you love \""
        ],
        [
         "18",
         "18",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" bitches get cut off everyday B \""
        ],
        [
         "19",
         "19",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" black bottle &amp; a bad bitch \""
        ],
        [
         "20",
         "20",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" broke bitch cant tell me nothing \""
        ],
        [
         "21",
         "21",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" cancel that bitch like Nino \""
        ],
        [
         "22",
         "22",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" cant you see these hoes wont change \""
        ],
        [
         "23",
         "23",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" fuck no that bitch dont even suck dick \" &#128514;&#128514;&#128514; the Kermit videos bout to fuck IG up"
        ],
        [
         "24",
         "24",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" got ya bitch tip toeing on my hardwood floors \" &#128514; http://t.co/cOU2WQ5L4q"
        ],
        [
         "25",
         "25",
         "3",
         "0",
         "2",
         "1",
         "1",
         "\" her pussy lips like Heaven doors \" &#128524;"
        ],
        [
         "26",
         "26",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" hoe what its hitting for \""
        ],
        [
         "27",
         "27",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" i met that pussy on Ocean Dr . i gave that pussy a pill \" &#128524;"
        ],
        [
         "28",
         "28",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" i need a trippy bitch who fuck on Hennessy \""
        ],
        [
         "29",
         "29",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" i spend my money how i want bitch its my business \""
        ],
        [
         "30",
         "30",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" i txt my old bitch my new bitch pussy wetter \""
        ],
        [
         "31",
         "31",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" i'd say im back to the old me but my old bitches would get excited \" &#128524;"
        ],
        [
         "32",
         "32",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" if you aint bout that Murder Game pussy nigga shut up \""
        ],
        [
         "33",
         "33",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" if you're toes ain't done you pussy stinks \""
        ],
        [
         "34",
         "34",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" im done with bitter bitches its a wrap for that . if you a angry bird theres a app for that \""
        ],
        [
         "35",
         "35",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" is that ya bitch \""
        ],
        [
         "36",
         "36",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" it aint nothing to cut a bitch off \""
        ],
        [
         "37",
         "37",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" jus meet son now he ya mane ass bitches \" #Shots"
        ],
        [
         "38",
         "38",
         "3",
         "0",
         "2",
         "1",
         "1",
         "\" lames crying over hoes thats tears of a clown \""
        ],
        [
         "39",
         "39",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" like Snoop said in 94 we dont love these hoes \""
        ],
        [
         "40",
         "40",
         "3",
         "0",
         "1",
         "2",
         "2",
         "\" momma said no pussy cats inside my doghouse \""
        ],
        [
         "41",
         "41",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" most hated but the hoes favorite \" #2MW #SevenOne # http://t.co/BMdSVMc3rC"
        ],
        [
         "42",
         "42",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" nice girls bad, make me get naughty. Bad yello hoe, real nice body. Down south chick, like em real thick\" http://t.co/bzRDl3kF7U"
        ],
        [
         "43",
         "43",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" pimps up pimps up hoes down \" Future voice"
        ],
        [
         "44",
         "44",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" post a picture of that pussy get 200 likes \""
        ],
        [
         "45",
         "45",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" pussy is a powerful drug \" &#128517; #HappyHumpDay http://t.co/R8jsymiB5b"
        ],
        [
         "46",
         "46",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" quick piece of pussy call it a drive by \""
        ],
        [
         "47",
         "47",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" running round here like some brand new pussy thats bout to get fucked \""
        ],
        [
         "48",
         "48",
         "3",
         "0",
         "3",
         "0",
         "1",
         "\" these bitches even worst they'll send them guys for you \""
        ],
        [
         "49",
         "49",
         "3",
         "1",
         "2",
         "0",
         "1",
         "\" these hoes like niggas that spend money not talk bout it \""
        ]
       ],
       "shape": {
        "columns": 7,
        "rows": 24783
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24778</th>\n",
       "      <td>25291</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>you's a muthaf***in lie &amp;#8220;@LifeAsKing: @2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24779</th>\n",
       "      <td>25292</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>you've gone and broke the wrong heart baby, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24780</th>\n",
       "      <td>25294</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>young buck wanna eat!!.. dat nigguh like I ain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24781</th>\n",
       "      <td>25295</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>youu got wild bitches tellin you lies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24782</th>\n",
       "      <td>25296</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>~~Ruffled | Ntac Eileen Dahlia - Beautiful col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24783 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  count  hate_speech  offensive_language  neither  class  \\\n",
       "0               0      3            0                   0        3      2   \n",
       "1               1      3            0                   3        0      1   \n",
       "2               2      3            0                   3        0      1   \n",
       "3               3      3            0                   2        1      1   \n",
       "4               4      6            0                   6        0      1   \n",
       "...           ...    ...          ...                 ...      ...    ...   \n",
       "24778       25291      3            0                   2        1      1   \n",
       "24779       25292      3            0                   1        2      2   \n",
       "24780       25294      3            0                   3        0      1   \n",
       "24781       25295      6            0                   6        0      1   \n",
       "24782       25296      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \n",
       "0      !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1      !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2      !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3      !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4      !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  \n",
       "...                                                  ...  \n",
       "24778  you's a muthaf***in lie &#8220;@LifeAsKing: @2...  \n",
       "24779  you've gone and broke the wrong heart baby, an...  \n",
       "24780  young buck wanna eat!!.. dat nigguh like I ain...  \n",
       "24781              youu got wild bitches tellin you lies  \n",
       "24782  ~~Ruffled | Ntac Eileen Dahlia - Beautiful col...  \n",
       "\n",
       "[24783 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "url = \"https://raw.githubusercontent.com/t-davidson/hate-speech-and-offensive-language/master/data/labeled_data.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15632/2424380511.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['tweet'] = df['tweet'].apply(clean_text)\n"
     ]
    }
   ],
   "source": [
    "# Clean + relabel\n",
    "df['label'] = df['class'].apply(lambda x: 1 if x in [0, 1] else 0)\n",
    "df = df[['tweet', 'label']]\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)\n",
    "    return text.lower().strip()\n",
    "\n",
    "df['tweet'] = df['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['tweet'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset class\n",
    "class ToxicCommentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels.tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = ToxicCommentDataset(train_texts, train_labels, tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_dataset = ToxicCommentDataset(test_texts, test_labels, tokenizer)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 2: Odd Layer vs Even Layer Student Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "\n",
    "# Load the teacher (12-layer BERT)\n",
    "teacher = BertModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "teacher.eval()\n",
    "\n",
    "# Define a 6-layer student using the same config\n",
    "student_config = BertConfig.from_pretrained(\"bert-base-uncased\", num_hidden_layers=6)\n",
    "student = BertModel(student_config)\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "teacher.to(device)\n",
    "student.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "def distillation_step(input_ids, attention_mask, layer_ids, teacher, student):\n",
    "    with torch.no_grad():\n",
    "        teacher_outputs = teacher(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        teacher_hidden = teacher_outputs.hidden_states\n",
    "\n",
    "    student_outputs = student(input_ids=input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "    student_hidden = student_outputs.hidden_states\n",
    "\n",
    "    loss = 0.0\n",
    "    for student_layer_idx, teacher_layer_idx in enumerate(layer_ids):\n",
    "        teacher_rep = teacher_hidden[teacher_layer_idx]\n",
    "        student_rep = student_hidden[student_layer_idx + 1]  # skip embeddings\n",
    "        loss += loss_fn(student_rep, teacher_rep)\n",
    "\n",
    "    return loss / len(layer_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_student(layer_ids, student, teacher, name=\"student_model\", num_epochs=5):\n",
    "    optimizer = Adam(student.parameters(), lr=5e-5)\n",
    "    student.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = distillation_step(input_ids, attention_mask, layer_ids, teacher, student)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(train_dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(student.state_dict(), f\"{name}.pt\")\n",
    "    print(f\"✅ Saved: {name}.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.3533\n",
      "Epoch 2/5 | Loss: 0.2361\n",
      "Epoch 3/5 | Loss: 0.1856\n",
      "Epoch 4/5 | Loss: 0.1510\n",
      "Epoch 5/5 | Loss: 0.1279\n",
      "✅ Saved: student_odd.pt\n",
      "Epoch 1/5 | Loss: 0.3733\n",
      "Epoch 2/5 | Loss: 0.2494\n",
      "Epoch 3/5 | Loss: 0.1924\n",
      "Epoch 4/5 | Loss: 0.1522\n",
      "Epoch 5/5 | Loss: 0.1242\n",
      "✅ Saved: student_even.pt\n"
     ]
    }
   ],
   "source": [
    "# Train using ODD teacher layers\n",
    "odd_layer_ids = [1, 3, 5, 7, 9, 11]\n",
    "student_odd = BertModel(student_config).to(device)\n",
    "train_student(odd_layer_ids, student_odd, teacher, name=\"student_odd\")\n",
    "\n",
    "# Train using EVEN teacher layers\n",
    "even_layer_ids = [2, 4, 6, 8, 10, 12]\n",
    "student_even = BertModel(student_config).to(device)\n",
    "train_student(even_layer_ids, student_even, teacher, name=\"student_even\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 3: LoRA Fine-Tuning for Student Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pip install peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Load 6-layer BERT student model for classification\n",
    "student_config.num_hidden_layers = 6\n",
    "student_model = BertForSequenceClassification(student_config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not load bitsandbytes native library: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/pk_124960/Desktop/PK_ait/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/pk_124960/Desktop/PK_ait/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/home/pk_124960/Desktop/PK_ait/.venv/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 72, in get_native_library\n",
      "    dll = ct.cdll.LoadLibrary(str(binary_path))\n",
      "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 452, in LoadLibrary\n",
      "    return self._dlltype(name)\n",
      "  File \"/usr/lib/python3.10/ctypes/__init__.py\", line 374, in __init__\n",
      "    self._handle = _dlopen(self._name, mode)\n",
      "OSError: /lib/x86_64-linux-gnu/libstdc++.so.6: version `GLIBCXX_3.4.32' not found (required by /home/pk_124960/Desktop/PK_ait/.venv/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 148,994 || all params: 67,105,540 || trainable%: 0.2220\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_CLS\n",
    ")\n",
    "\n",
    "# Inject LoRA into the student model\n",
    "lora_student = get_peft_model(student_model, lora_config)\n",
    "lora_student.print_trainable_parameters()  # optional: verify only adapters are trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk_124960/Desktop/PK_ait/.venv/lib/python3.10/site-packages/transformers/optimization.py:640: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 1240/1240 [17:27<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | LoRA Fine-tuning Loss: 0.3210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1240/1240 [17:28<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 | LoRA Fine-tuning Loss: 0.1814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1240/1240 [17:29<00:00,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 | LoRA Fine-tuning Loss: 0.1554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "optimizer = AdamW(lora_student.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "lora_student.train()\n",
    "\n",
    "for epoch in range(3):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = lora_student(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1} | LoRA Fine-tuning Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA model saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk_124960/Desktop/PK_ait/.venv/lib/python3.10/site-packages/peft/utils/save_and_load.py:227: UserWarning: Could not find a config file in  - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "lora_student.save_pretrained(\"student_lora\")\n",
    "print(\"✅ LoRA model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from transformers import BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            true_labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "            preds.extend(predictions.cpu().numpy())\n",
    "            labels.extend(true_labels.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "\n",
    "    return {\"accuracy\": acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15632/3844819315.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student_odd_model.load_state_dict(torch.load(\"student_odd.pt\", map_location=device), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Odd Layer Student: {'accuracy': 0.1684486584627799, 'precision': 0.0, 'recall': 0.0, 'f1': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pk_124960/Desktop/PK_ait/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Odd Student Model\n",
    "student_odd_model = BertForSequenceClassification(student_config)\n",
    "student_odd_model.load_state_dict(torch.load(\"student_odd.pt\", map_location=device), strict=False)\n",
    "student_odd_model.to(device)\n",
    "\n",
    "results_odd = evaluate_model(student_odd_model, test_dataloader)\n",
    "print(\"Odd Layer Student:\", results_odd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_15632/2369071335.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  student_even_model.load_state_dict(torch.load(\"student_even.pt\", map_location=device), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Layer Student: {'accuracy': 0.8315513415372201, 'precision': 0.8315513415372201, 'recall': 1.0, 'f1': 0.908029518669457}\n"
     ]
    }
   ],
   "source": [
    "# Even Student Model\n",
    "student_even_model = BertForSequenceClassification(student_config)\n",
    "student_even_model.load_state_dict(torch.load(\"student_even.pt\", map_location=device), strict=False)\n",
    "student_even_model.to(device)\n",
    "\n",
    "results_even = evaluate_model(student_even_model, test_dataloader)\n",
    "print(\"Even Layer Student:\", results_even)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Model: {'accuracy': 0.8234819447246319, 'precision': 0.8309887869520897, 'recall': 0.9888403687530325, 'f1': 0.903068572061593}\n"
     ]
    }
   ],
   "source": [
    "# LoRA Model\n",
    "from peft import PeftModel, PeftConfig\n",
    "\n",
    "lora_config = PeftConfig.from_pretrained(\"student_lora\")\n",
    "base_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_hidden_layers=6)\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"student_lora\")\n",
    "lora_model.to(device)\n",
    "\n",
    "results_lora = evaluate_model(lora_model, test_dataloader)\n",
    "print(\"LoRA Model:\", results_lora)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Performance Comparison:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "f1",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "ecdf308a-d645-4ff7-bf64-10a089f9e5a2",
       "rows": [
        [
         "0",
         "Odd Layer",
         "0.1684486584627799",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "1",
         "Even Layer",
         "0.8315513415372201",
         "0.8315513415372201",
         "1.0",
         "0.908029518669457"
        ],
        [
         "2",
         "LoRA",
         "0.8234819447246319",
         "0.8309887869520897",
         "0.9888403687530325",
         "0.903068572061593"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Odd Layer</td>\n",
       "      <td>0.168449</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Even Layer</td>\n",
       "      <td>0.831551</td>\n",
       "      <td>0.831551</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.908030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LoRA</td>\n",
       "      <td>0.823482</td>\n",
       "      <td>0.830989</td>\n",
       "      <td>0.98884</td>\n",
       "      <td>0.903069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Model  accuracy  precision   recall        f1\n",
       "0   Odd Layer  0.168449   0.000000  0.00000  0.000000\n",
       "1  Even Layer  0.831551   0.831551  1.00000  0.908030\n",
       "2        LoRA  0.823482   0.830989  0.98884  0.903069"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "comparison_df = pd.DataFrame([\n",
    "    {\"Model\": \"Odd Layer\", **results_odd},\n",
    "    {\"Model\": \"Even Layer\", **results_even},\n",
    "    {\"Model\": \"LoRA\", **results_lora}\n",
    "])\n",
    "\n",
    "print(\"\\n Performance Comparison:\")\n",
    "display(comparison_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
